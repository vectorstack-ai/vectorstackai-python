{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this tutorial, we will build a \"Hybrid Search\" application that retrieves the most relevant passages for a given query. \n",
    "\n",
    "### Why Hybrid Search?\n",
    "Traditional search setups typically rely on either dense or sparse embeddings, each with its own strengths:\n",
    "\n",
    "- **Dense embeddings** capture semantic meaning, helping the model understand contextual similarities.\n",
    "- **Sparse embeddings** emphasize explicit keyword matches, making them useful for handling rare or domain-specific terms.\n",
    "\n",
    "Hybrid search leverages both approaches, allowing for a more balanced and effective retrieval system. By adjusting the weight of each component, you can fine-tune search performance to better suit your needs.\n",
    "\n",
    "### When is Hybrid Search Useful?\n",
    "\n",
    "A common limitation of using only dense embeddings is that they may struggle with rare or domain-specific terms. \n",
    "Since dense models focus on semantic meaning, they might overlook **crucial keywords** that are \n",
    "essential for certain types of searches.\n",
    "\n",
    "Hybrid search is especially useful when:\n",
    "\n",
    "- You are using pre-trained embedding models to generate dense representations.\n",
    "- Your text data contains a high number of rare or domain-specific terms (e.g., medical manuals, legal documents).\n",
    "\n",
    "For example, in legal or medical texts, dense embeddings alone may struggle to distinguish between highly specific technical terms, reducing search precision.\n",
    "\n",
    "By combining dense embeddings for contextual understanding with sparse embeddings for keyword precision, hybrid search ensures more accurate and reliable retrieval—capturing both meaning and specificity in search results.\n",
    "\n",
    "\n",
    "\n",
    "### Implementation Steps\n",
    "1. Setup Knowledge Base \n",
    "    - We will use the [MS MARCO dataset](https://huggingface.co/datasets/microsoft/ms_marco), which contains real Bing search queries paired with relevant passages to construct a knowledge base.\n",
    "2. Embedding Generation\n",
    "    - Dense Embeddings – We will rely on PreciseSearch's integrated embedding model to generate dense embeddings for the queries and passages.\n",
    "    - Sparse Embeddings – We will use the `sparse_embedding` function to generate sparse embeddings for the queries and passages.\n",
    "3. Indexing for Search \n",
    "    - We will build a hybrid search index with PreciseSearch's SDK for fast and accurate retrieval using both dense and sparse embeddings.\n",
    "\n",
    "By the end, you'll have a fully functional search system capable of understanding natural language queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "To get started, install the required Python packages:\n",
    "\n",
    "- `datasets`: A library to load and process the MS MARCO dataset.\n",
    "- `sentence-transformers`: Sentence Transformers Python SDK for generating dense embeddings for queries and passages.\n",
    "- `vectorstackai`: A package for building and querying semantic search indexes. It provides seamless integration with PreciseSearch, a high-performance search solution from [VectorStackAI](https://vectorstack.ai), designed for efficient and accurate retrieval.\n",
    "\n",
    "\n",
    "Run the following command to install the packages (note this may take some time):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentence-transformers datasets vectorstackai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and prepare the dataset (MS MARCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the dataset (MS MARCO) from Hugging Face\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  was ronald reagan a democrat\n",
      "List of passages: \n",
      "In his younger years, Ronald Reagan was a member of the Democratic Party and campaigned for Democratic candidates; however, his views grew more conservative over time, and in the early 1960s he officially became a Republican. In November 1984, Ronald Reagan was reelected in a landslide, defeating Walter Mondale and his running mate Geraldine Ferraro (1935-), the first female vice-presidential candidate from a major U.S. political party.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "From Wikipedia, the free encyclopedia. A Reagan Democrat is a traditionally Democratic voter in the United States, especially a white working-class Northerner, who defected from their party to support Republican President Ronald Reagan in either or both the 1980 and 1984 elections. During the 1980 election a dramatic number of voters in the U.S., disillusioned with the economic 'malaise' of the 1970s and the presidency of Jimmy Carter (even more than, four years earlier, Liberal Republican Gerald Ford), supported former California governor (and former Democrat) Ronald Reagan.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ronald Reagan began his political life in the Democratic Party, but as he became more and more conservative, he ultimately changed to the Republican Party in the early 1960s. Yes, he switched parties in 1962. He said that he did not desert the Democrats but rather they deserted him. Yes, Ronald Reagan was a member of the Democratic Party until he s … witched to the Republican Party in 1962, at the age of 51. 8 people found this useful.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ronald Wilson Reagan (/ˈrɒnəld ˈwɪlsən ˈreɪɡən/ ; February 6, 1911 – June 5, 2004) was an American politician, commentator, and actor, who served as the 40th President of the United States from 1981 to 1989. I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "When Reagan was a 'liberal Democrat'. In 1948, a very different sounding Ronald Reagan campaigned on the radio for Democrat Harry Truman. Listen to the old audio recording. ... more Duration: {{video.duration.momentjs}}. \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ronald Reagan (1911-2004), a former actor and California governor, served as the 40th U.S. president from 1981 to 1989. Raised in small-town Illinois, he became a Hollywood actor in his 20s and later served as the Republican governor of California from 1967 to 1975. In November 1984, Ronald Reagan was reelected in a landslide, defeating Walter Mondale and his running mate Geraldine Ferraro (1935-), the first female vice-presidential candidate from a major U.S. political party.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1984 Re-Election. In November 1984, Ronald Reagan was re-elected in a landslide, defeating Democratic challenger Walter Mondale. Reagan carried 49 of the 50 U.S. states in the election, and received 525 of 538 electoral votes—the largest number ever won by an American presidential candidate. \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We will use the first 100 samples from the training set\n",
    "ds_tutorial = ds['train'].select(range(100))\n",
    "\n",
    "sample = ds_tutorial[1]\n",
    "\n",
    "# Each sample contains:\n",
    "# query: the question\n",
    "# list_of_passages: a list of passages which are relevant to the question\n",
    "query = sample['query']\n",
    "list_of_passages = sample['passages']['passage_text']\n",
    "\n",
    "print('Query: ', query)\n",
    "print('List of passages: ')\n",
    "for passage in list_of_passages:\n",
    "    print(passage)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected  814  passages for our knowledge base\n"
     ]
    }
   ],
   "source": [
    "# We will collect all the passage texts to make our knowledge base\n",
    "knowledge_base = []\n",
    "for sample in ds_tutorial:\n",
    "    list_of_passages = sample['passages']['passage_text']\n",
    "    knowledge_base.append(list_of_passages)\n",
    "\n",
    "# Flatten the list of passages\n",
    "knowledge_base = [item for sublist in knowledge_base for item in sublist]\n",
    "\n",
    "print('Collected ', len(knowledge_base), ' passages for our knowledge base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate dense and sparse embeddings for the knowledge base\n",
    "\n",
    "Now that we have our knowledge base, we need to generate both dense and sparse embeddings for each passage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Embeddings\n",
    "\n",
    "We will use OpenAI's API to generate dense embeddings for each passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for the knowledge base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 39.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation is done!\n",
      "Generated embeddings for 814 passages\n",
      "Dimensions of the embeddings: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Instantiate the MiniLM-L6 model from sentence-transformers \n",
    "model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L6-cos-v5')\n",
    "\n",
    "# Generate embeddings for the knowledge base\n",
    "print('Generating embeddings for the knowledge base...')\n",
    "dense_embeddings = model.encode(knowledge_base, show_progress_bar=True)\n",
    "\n",
    "# Convert from numpy array to List[List[float]]\n",
    "dense_embeddings = dense_embeddings.tolist()\n",
    "\n",
    "\n",
    "print('Embedding generation is done!')\n",
    "# Embeddings are stored in the `dense_embeddings` numpy array\n",
    "print(f'Generated embeddings for {len(dense_embeddings)} passages') \n",
    "print(f'Dimensions of the embeddings: {len(dense_embeddings[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Embeddings\n",
    "\n",
    "Now, we will generate sparse embeddings for the knowledge base.\n",
    "In general, there are multiple ways to generate sparse embeddings, from BM25 to TF-IDF, to recent state-of-the-art transformer-based models like SPLADE. All of these models transform the text into a sparse vector space.\n",
    "\n",
    "To keep things simple in this tutorial, we will use BERT based tokenizer to generate sparse embeddings.\n",
    "\n",
    "**Important**: \n",
    "\n",
    "Regardless of the method used to generate the sparse embeddings, PreciseSearch expects the sparse embeddings to be represented by two lists:\n",
    "- sparse_indices List[int]: contains the token indices for the corresponding text.\n",
    "- sparse_values List[float]: contains the frequency of each token in the corresponding text.\n",
    "\n",
    "For example, the sparse vector [0, 0, 1.2, 0 , 2.3] can be represented by:\n",
    "- sparse_indices = [2, 4]\n",
    "- sparse_values = [1.2, 2.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def generate_sparse_embeddings_with_bert_tokenizer(list_of_texts):\n",
    "    \"\"\"\n",
    "    Generate sparse embeddings for a list of texts using the BERT tokenizer.\n",
    "    \n",
    "    This function tokenizes input texts using the BERT tokenizer and constructs sparse \n",
    "    vector representations based on token frequency. Each token ID serves as an index \n",
    "    in the sparse vector, and its frequency in the text determines its corresponding value.\n",
    "\n",
    "    Args:\n",
    "        list_of_texts (List[str]): A list of textual inputs to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        list_of_sparse_indices (List[List[int]]): A list where each sublist contains \n",
    "            unique token indices for the corresponding text.\n",
    "        list_of_sparse_values (List[List[float]]): A list where each sublist contains \n",
    "            the frequency of each token in the corresponding text.\n",
    "    \"\"\"\n",
    "    # Load the BERT tokenizer (pretrained on 'bert-base-uncased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Lists to store sparse representations for each text\n",
    "    list_of_sparse_indices = []\n",
    "    list_of_sparse_values = []\n",
    "\n",
    "    for text in list_of_texts:\n",
    "        # Tokenize the text without padding, truncating to a maximum of 512 tokens\n",
    "        output = tokenizer(text, padding=False, truncation=True, max_length=512, return_tensors='np')\n",
    "        token_ids = output['input_ids'][0]  # Extract token IDs (list of integers)\n",
    "\n",
    "        # Count occurrences of each token ID in the text\n",
    "        token_counts = Counter(token_ids)\n",
    "\n",
    "        # Store the token indices and their respective counts (sparse representation)\n",
    "        list_of_sparse_indices.append([int(token_id) for token_id in list(token_counts.keys())])  # Unique token IDs (sparse indices)\n",
    "        list_of_sparse_values.append(list(token_counts.values()))  # Corresponding counts (sparse values)\n",
    "\n",
    "    return list_of_sparse_indices, list_of_sparse_values\n",
    "\n",
    "# Generate sparse embeddings for the knowledge base\n",
    "sparse_indices_knowledge_base, sparse_values_knowledge_base = generate_sparse_embeddings_with_bert_tokenizer(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a search index\n",
    "\n",
    "With our knowledge base and its dense and sparse embeddings ready, it's time to build a search index for efficient retrieval.\n",
    "We'll use **PreciseSearch** to create a high-performance index, enabling fast and accurate passage retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request accepted: Index creation for 'ms_marco_hybrid_index' started.\n",
      "Index is not ready yet. Waiting for 2 seconds...\n",
      "Connected to the index\n"
     ]
    }
   ],
   "source": [
    "from vectorstackai import Client\n",
    "import time\n",
    "\n",
    "precise_search_client = Client(\n",
    "    api_key=\"temp_api_key\"\n",
    "    ) # Replace with your own PreciseSearch API key\n",
    "\n",
    "# Create the index \n",
    "precise_search_client.create_index(index_name=\"ms_marco_hybrid_index\", \n",
    "                                   dimension=len(dense_embeddings[0]), # The dimension of the dense embeddings\n",
    "                                   metric=\"dotproduct\", # The metric to use for the search\n",
    "                                   features_type=\"hybrid\") # The type of features we are going to build the index on\n",
    "\n",
    "# Wait for the index to be ready\n",
    "while precise_search_client.get_index_info(index_name=\"ms_marco_hybrid_index\")['status'] != \"ready\":\n",
    "    time.sleep(2)\n",
    "    print(\"Index is not ready yet. Waiting for 2 seconds...\")\n",
    "    \n",
    "# Connect to the index\n",
    "index = precise_search_client.connect_to_index(index_name=\"ms_marco_hybrid_index\")\n",
    "print('Connected to the index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload the dense and sparse embeddings to the index\n",
    "\n",
    "Upsert in a hybrid index requires a batch of ids, dense vectors and sparse vectors\n",
    "\n",
    "- Required inputs:\n",
    "  - batch_ids (List[str]): Unique identifiers for each passage.\n",
    "  - batch_vectors (List[List[float]]): Embeddings for each passage.\n",
    "  - batch_sparse_indices (List[List[int]]): Sparse indices for each passage.\n",
    "  - batch_sparse_values (List[List[float]]): Sparse values for each passage.\n",
    "- Optional inputs:\n",
    "  - batch_metadata (List[Dict[str, Any]]): Metadata for each passage, can store any additional information about the items in the batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings uploaded to the index\n"
     ]
    }
   ],
   "source": [
    "# Convert the data in the required format\n",
    "batch_ids = [str(i) for i in list(range(len(knowledge_base)))]\n",
    "batch_vectors = dense_embeddings\n",
    "batch_sparse_indices = sparse_indices_knowledge_base\n",
    "batch_sparse_values = sparse_values_knowledge_base\n",
    "batch_metadata = [{\"text\": passage} for passage in knowledge_base]\n",
    "\n",
    "index.upsert(batch_ids=batch_ids, \n",
    "             batch_vectors=batch_vectors, \n",
    "             batch_sparse_values=batch_sparse_values,\n",
    "             batch_sparse_indices=batch_sparse_indices,\n",
    "             batch_metadata=batch_metadata)\n",
    "print('Embeddings uploaded to the index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Querying the index\n",
    "\n",
    "Now that we have uploaded the knowledge base and its embeddingsto the index, we can query it.\n",
    "\n",
    "Given an input query as a text, we will first compute the dense and sparse embeddings for the query and then search for the most relevant passages in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n",
      "13.2890625\n",
      "{'text': 'Cook Pork and Pork Products to an internal temperature of at least 155°F. Pork ordered medium should be cooked to at least 155°F. Pork ordered well done should be cooked to at least 170°F. Temperatures should be taken at the thickest portion of pork. Meat should be firm, not mushy. Juices should be clear, not pink. Cook Eggs and Foods Containing Raw Eggs to an internal temperature of at least 145°F. This requirement does not apply to foods made with pasteurized eggs. Temperatures should be taken in the center of the egg-containing food. Cooked egg whites and yolks should be firm after cooking, not runny..'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "663\n",
      "11.609375\n",
      "{'text': 'Kitchen Fact: Cooked food stored in the refrigerator should be eaten in 3 to 4 days. After food is cooked, it should sit out at room temperature no more than two hours before being refrigerated to slow down bacteria growth. But once stored in the fridge, leftovers should be eaten up within three to four days because bacteria can still grow even at refrigerator temperatures. To help you remember how long something has been in the fridge, we recommend labeling it with the date it was cooked to help you keep track!'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "664\n",
      "10.65625\n",
      "{'text': 'Cooked Food: Leftover, cooked foods should be kept in the refrigerator in an airtight container and eaten within 4-5 days. Food, whether cooked or not, should not be left at room temperature for more than 4 hours otherwise the risk of food poisoning increases. You can refrigerate them for up to 5 days as long as they are stored properly — wrapped in a paper towel and then sealed inside a plastic bag. Fresh Beans/Peas: Depending on the variety, they can be kept, tightly wrapped, in the refrigerator for 3-5 days.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "666\n",
      "8.5703125\n",
      "{'text': \"Uncooked foods, such as cold salads or sandwiches, also should be eaten or refrigerated promptly. Your goal is to minimize the time a food is in the danger zone — between 40 and 140 F (4 and 60 C) — when bacteria can quickly multiply. When you're ready to eat leftovers, reheat them on the stove, in the oven or in the microwave until the internal temperature reaches 165 F (74 C). After that, the risk of food poisoning increases. If you don't think you'll be able to eat leftovers within four days, freeze them immediately. Food poisoning — also called foodborne illness — is caused by harmful organisms, such as bacteria in contaminated food.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "290\n",
      "8.328125\n",
      "{'text': 'Report Abuse. Bacteria need time, food and moisture to grow. They do not grow well unless they are in the danger zone which is between 40-140. Some bacteria are found naturally in food. They can be killed if the food is cooked to the correct temperature. BUT.....some bacteria produce poisons. (when left in the danger zone) heating the food will kill the bacteria. However, the poisons will not be destroyed and can still cause a foodborne illness'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"How should I store cooked food\"\n",
    "\n",
    "# Compute the dense embedding for the query\n",
    "query_embedding = model.encode(query).tolist()\n",
    "\n",
    "# Compute the sparse embedding for the query\n",
    "query_sparse_indices, query_sparse_values = generate_sparse_embeddings_with_bert_tokenizer([query])\n",
    "\n",
    "# Get the first and only element from the list\n",
    "query_sparse_indices = query_sparse_indices[0]\n",
    "query_sparse_values = query_sparse_values[0]\n",
    "\n",
    "# Search for the most relevant passages\n",
    "search_results = index.search(query_vector=query_embedding, \n",
    "                              query_sparse_indices=query_sparse_indices,\n",
    "                              query_sparse_values=query_sparse_values,\n",
    "                              top_k=5\n",
    "                              )\n",
    "\n",
    "# Print the results\n",
    "for result in search_results:\n",
    "    print(result['id'])\n",
    "    print(result['similarity'])\n",
    "    print(result['metadata'])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search results are returned as a list of dictionaries, where each dictionary contains the following keys:\n",
    "\n",
    "- `id`: The identifier of the matching document/vector.\n",
    "- `similarity`: The similarity score for the match (higher is typically more relevant).\n",
    "- `metadata`: Any additional metadata stored with the vector (only returned if `return_metadata=True`).\n",
    "\n",
    "In the search results shown above, we can note the following:\n",
    "\n",
    "- The results are sorted by similarity score, meaning the first result is the most relevant.\n",
    "- The results are relevant to the query!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adjusting the importance of dense and sparse embeddings\n",
    "When searching in a hybrid index, the similarity score between a query and a document is computed as a weighted sum of the query–document similarity in both dense and sparse vector spaces:\n",
    "\n",
    "$$\n",
    "\\text{similarity} =  \\text{dense\\_similarity} \\cdot \\text{dense\\_scale} + \\text{sparse\\_similarity} \\cdot \\text{sparse\\_scale}\n",
    "$$\n",
    "\n",
    "where `dense_scale` and `sparse_scale` are the weights for the dense and sparse embeddings, respectively.\n",
    "By default, both weights are set to 1.0. You can change the weights by calling the `set_similarity_scale` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n",
      "6.79296875\n",
      "{'text': 'Cook Pork and Pork Products to an internal temperature of at least 155°F. Pork ordered medium should be cooked to at least 155°F. Pork ordered well done should be cooked to at least 170°F. Temperatures should be taken at the thickest portion of pork. Meat should be firm, not mushy. Juices should be clear, not pink. Cook Eggs and Foods Containing Raw Eggs to an internal temperature of at least 145°F. This requirement does not apply to foods made with pasteurized eggs. Temperatures should be taken in the center of the egg-containing food. Cooked egg whites and yolks should be firm after cooking, not runny..'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "663\n",
      "6.109375\n",
      "{'text': 'Kitchen Fact: Cooked food stored in the refrigerator should be eaten in 3 to 4 days. After food is cooked, it should sit out at room temperature no more than two hours before being refrigerated to slow down bacteria growth. But once stored in the fridge, leftovers should be eaten up within three to four days because bacteria can still grow even at refrigerator temperatures. To help you remember how long something has been in the fridge, we recommend labeling it with the date it was cooked to help you keep track!'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "664\n",
      "5.66015625\n",
      "{'text': 'Cooked Food: Leftover, cooked foods should be kept in the refrigerator in an airtight container and eaten within 4-5 days. Food, whether cooked or not, should not be left at room temperature for more than 4 hours otherwise the risk of food poisoning increases. You can refrigerate them for up to 5 days as long as they are stored properly — wrapped in a paper towel and then sealed inside a plastic bag. Fresh Beans/Peas: Depending on the variety, they can be kept, tightly wrapped, in the refrigerator for 3-5 days.'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "666\n",
      "4.56640625\n",
      "{'text': \"Uncooked foods, such as cold salads or sandwiches, also should be eaten or refrigerated promptly. Your goal is to minimize the time a food is in the danger zone — between 40 and 140 F (4 and 60 C) — when bacteria can quickly multiply. When you're ready to eat leftovers, reheat them on the stove, in the oven or in the microwave until the internal temperature reaches 165 F (74 C). After that, the risk of food poisoning increases. If you don't think you'll be able to eat leftovers within four days, freeze them immediately. Food poisoning — also called foodborne illness — is caused by harmful organisms, such as bacteria in contaminated food.\"}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "290\n",
      "4.328125\n",
      "{'text': 'Report Abuse. Bacteria need time, food and moisture to grow. They do not grow well unless they are in the danger zone which is between 40-140. Some bacteria are found naturally in food. They can be killed if the food is cooked to the correct temperature. BUT.....some bacteria produce poisons. (when left in the danger zone) heating the food will kill the bacteria. However, the poisons will not be destroyed and can still cause a foodborne illness'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#To change the weights, for example, to make dense embeddings twice as important as sparse embeddings, we can do the following:\n",
    "index.set_similarity_scale(dense_scale=1.0, sparse_scale=0.5)\n",
    "\n",
    "# Search again\n",
    "search_results = index.search(query_vector=query_embedding, \n",
    "                              query_sparse_indices=query_sparse_indices,\n",
    "                              query_sparse_values=query_sparse_values,\n",
    "                              top_k=5)\n",
    "\n",
    "# Print the results\n",
    "for result in search_results:\n",
    "    print(result['id'])\n",
    "    print(result['similarity'])\n",
    "    print(result['metadata'])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean up\n",
    "Feel free to experiment with different queries and weights to adjust the importance of dense and sparse embeddings.\n",
    "Once you are done with the tutorial, you can delete the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to delete index 'ms_marco_hybrid_index'? This action is irreversible.\n",
      "Index deletion for 'ms_marco_hybrid_index' started.\n"
     ]
    }
   ],
   "source": [
    "precise_search_client.delete_index(index_name=\"ms_marco_hybrid_index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temp_sentence_transformer)",
   "language": "python",
   "name": "temp_sentence_transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
