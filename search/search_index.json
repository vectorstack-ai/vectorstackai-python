{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"VectorStackAI: Vertically Integrated Generative AI Stack for Enterprises","text":"<p>VectorStackAI develops lean and efficient Generative AI solutions tailored for enterprise needs.</p>"},{"location":"index.html#offerings-by-vectorstackai","title":"Offerings by VectorStackAI","text":"<p>Explore documentation for our specialized solutions:</p> <ul> <li>PreciseSearch: A vertically integrated search-as-a-service solution, providing fast and accurate similarity search across large-scale datasets.</li> <li>Embeddings: Embeddings-as-a-service solution, providing state of the art domain specific embeddings.</li> </ul>"},{"location":"embeddings/index.html","title":"Embeddings Documentation","text":""},{"location":"embeddings/index.html#introduction","title":"Introduction","text":"<p>VectorStackAI provides state-of-the-art domain-specific embeddings optimized for high-precision retrieval and reasoning. Our models outperform generic embeddings by leveraging specialized training on high-quality, domain-specific datasets.</p> <p>The documentation is divided into the following sections:</p> <ol> <li>Quickstart &amp; Installation: Get started with VectorStackAI Embeddings in minutes, including installation and initial setup.</li> <li>Reference: API reference for the VectorStackAI Embeddings Python SDK.</li> </ol>"},{"location":"embeddings/quickstart.html","title":"Embeddings Quickstart","text":"<p>This guide will show you how to get started with VectorStackAI Embeddings.  You'll learn how to:   \u2705 Install the SDK    \u2705 Obtain an API key    \u2705 Generate embeddings for documents and queries    \u2705 Perform a similarity search</p> <p>Code for this quickstart</p> <p>You can find and run the code for this quickstart in this colab notebook</p>"},{"location":"embeddings/quickstart.html#1-install-the-sdk","title":"1. Install the SDK","text":"<p>VectorStackAI Embeddings is an embeddings-as-a-service product by VectorStackAI,  designed to provide state-of-the-art domain-specific embeddings.  Currently, we offer a Python SDK to interact with the VectorStackAI Embeddings service. To get started, install the VectorStackAI Python SDK using pip:</p> <pre><code>pip install vectorstackai\n</code></pre>"},{"location":"embeddings/quickstart.html#2-get-an-api-key","title":"2. Get an API key","text":"<p>You will need an API key to use the SDK.  You can get it by signing up on the VectorStackAI website.</p>"},{"location":"embeddings/quickstart.html#3-generating-embeddings","title":"3. Generating Embeddings","text":"<p>This section covers generating embeddings for both documents and queries.</p>"},{"location":"embeddings/quickstart.html#31-generating-embeddings-for-documents","title":"3.1 Generating Embeddings for Documents","text":"<p>To generate embeddings, first import the vectorstackai package and create a client object with your API key:</p> <pre><code>import vectorstackai\n\n# Replace with your actual API key\napi_key = \"your_api_key\"\nclient = vectorstackai.Client(api_key=api_key, timeout=30)\n</code></pre> <p>Once, the client object is created, you can use the <code>client.embed</code> method to generate embeddings for a list of documents.</p> <p>The method takes the following parameters:</p> <ul> <li><code>texts</code>: A list of text documents to embed</li> <li><code>model</code>: The name of the embedding model to use (e.g. 'vstackai-law-1' for legal documents)</li> <li><code>is_query</code>: A boolean flag indicating whether the texts are queries (<code>True</code>) or documents (<code>False</code>)</li> </ul> <p>The method returns an <code>EmbeddingObject</code> containing the generated embeddings. The embeddings are in numpy array format, and can be accessed using the <code>embeddings</code> attribute of the <code>EmbeddingObject</code>.</p> <p>For more details on the <code>embed</code> method, checkout the API reference here.</p> <pre><code># Documents related to law domain (e.g., court cases, consumer contracts, etc.)\ndocuments = [\n    \"The defendant was charged with violation of contract terms in the lease agreement signed on January 1, 2022.\",\n    \"This contract stipulates that the consumer has 30 days to return the product in case of any manufacturing defects.\",\n    \"In the case of Smith v. Johnson, the court ruled that the plaintiff had the right to claim damages under section 12 of the Consumer Protection Act.\"\n]\n\n# Get embeddings for the legal documents\ndoc_embeddings = client.embed(texts=documents, model='vstackai-law-1', is_query=False)\ndoc_embeddings = doc_embeddings.embeddings  # (3, 1536) numpy array\n</code></pre>"},{"location":"embeddings/quickstart.html#32-generating-embeddings-for-queries","title":"3.2 Generating Embeddings for Queries","text":"<p>Now, let's generate embeddings for a query.</p> <p>Since <code>vstackai-law-1</code> is an instruction-tuned model, it is recommended to provide an instruction when embedding queries. This helps guide the model to produce embeddings that are more relevant to the task/instruction. You can learn more about instruction-tuned models here.</p> <pre><code># Encode a query\nquery = \"How many days does the consumer have to return the product?\"\nquery_embedding = client.embed(\n    texts=[query], \n    model='vstackai-law-1', \n    is_query=True, \n    instruction='Represent the query for searching legal documents'\n)\nquery_embedding = query_embedding.embeddings  # (1, 1536) numpy array\n</code></pre>"},{"location":"embeddings/quickstart.html#4-computing-similarity","title":"4. Computing Similarity","text":"<p>Once you have embeddings for both documents and queries, you can compute similarity scores to find the most relevant match.</p> <p>Below, we compute the dot product of the document embeddings and the query embedding to get the similarity scores. You can use other similarity metrics as well (eg. cosine similarity, euclidean distance, etc.).</p> <pre><code># Compute similarity between query and documents\nsimilarities = np.dot(doc_embeddings, query_embedding.T)\nprint(similarities)\n# Example output:\n# array([[0.355],\n#        [0.772],\n#        [0.433]])\n</code></pre> <p>The document with the highest similarity score corresponds to the most relevant match for the query.</p>"},{"location":"embeddings/quickstart.html#5-batch-size-limits","title":"5. Batch Size Limits","text":"<p>For optimal performance, it is recommended to generate embeddings by batching multiple texts at once (as shown in the examples above).  Batching helps reduce the number of API calls and improves throughput.</p> <p>Batch Size Limits</p> <p>There are limits to the number of texts you can embed in a single request:</p> <ul> <li><code>vstackai-law-1</code>: Maximum of 64 texts per batch</li> </ul>"},{"location":"embeddings/quickstart.html#6-conclusion","title":"6. Conclusion","text":"<p>This concludes the quickstart guide. You can now use the VectorStackAI Embeddings service to generate embeddings for your documents and queries.</p>"},{"location":"embeddings/reference.html","title":"VectorStackAI's Embeddings API Reference","text":""},{"location":"embeddings/reference.html#embeddings-operations","title":"Embeddings Operations","text":""},{"location":"embeddings/reference.html#vectorstackai.Client.embed","title":"vectorstackai.Client.embed","text":"<pre><code>embed(texts, model, is_query=False, instruction='')\n</code></pre> <p>Generates embeddings for a batch of text inputs using the specified model.</p> <p>This method encodes a batch of text documents or queries into dense vector  representations using the selected embedding model. It supports both  document and query embeddings, with an optional instruction for  instruction-tuned models.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>Batch of text strings to be embedded as a list of strings.  Each string represents either a document or a query.</p> required <code>model</code> <code>str</code> <p>The name of the embedding model to use (e.g., <code>\"vstackai-law-1\"</code>  for legal documents).</p> required <code>is_query</code> <code>bool</code> <p>A flag indicating whether the input texts are queries (<code>True</code>)  or documents (<code>False</code>). Defaults to <code>False</code>.</p> <code>False</code> <code>instruction</code> <code>str</code> <p>An optional instruction to guide the model when embedding queries.  Recommended for instruction-tuned models. Defaults to an empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>EmbeddingsObject</code> <code>EmbeddingsObject</code> <p>An object that holds embeddings for the batch of texts.  The embeddings are stored as a NumPy array of shape <code>(num_texts, embedding_dimension)</code>, accesible via the  <code>embeddings</code> attribute.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>texts</code> is not a list of strings. If <code>model</code> is not a string. If <code>is_query</code> is not a boolean. If <code>instruction</code> is not a string.</p> Example <pre><code>client = vectorstackai.Client(api_key=\"your_api_key\")\n\ntexts = [\n    \"The defendant was charged with violation of contract terms.\",\n    \"Consumers have 30 days to return a defective product.\"\n]\n\nembeddings = client.embed(texts=texts, model=\"vstackai-law-1\", is_query=False)\n\nprint(embeddings.embeddings.shape)  # (2, 1536)\n</code></pre> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def embed(\n    self,\n    texts: List[str],\n    model: str,\n    is_query: bool = False,\n    instruction: str = \"\",\n) -&gt; EmbeddingsObject:\n    \"\"\"\n    Generates embeddings for a batch of text inputs using the specified model.\n\n    This method encodes a batch of text documents or queries into dense vector \n    representations using the selected embedding model. It supports both \n    document and query embeddings, with an optional instruction for \n    instruction-tuned models.\n\n    Args:\n        texts (List[str]): \n            Batch of text strings to be embedded as a list of strings. \n            Each string represents either a document or a query.\n        model (str): \n            The name of the embedding model to use (e.g., `\"vstackai-law-1\"` \n            for legal documents).\n        is_query (bool, optional): \n            A flag indicating whether the input texts are queries (`True`) \n            or documents (`False`). Defaults to `False`.\n        instruction (str, optional): \n            An optional instruction to guide the model when embedding queries. \n            Recommended for instruction-tuned models. Defaults to an empty string.\n\n    Returns:\n        EmbeddingsObject: \n            An object that holds embeddings for the batch of texts. \n            The embeddings are stored as a NumPy array of shape\n            `(num_texts, embedding_dimension)`, accesible via the \n            `embeddings` attribute.\n\n    Raises:\n        ValueError: \n            If `texts` is not a list of strings.\n            If `model` is not a string.\n            If `is_query` is not a boolean.\n            If `instruction` is not a string.\n\n    Example:\n        ```python\n        client = vectorstackai.Client(api_key=\"your_api_key\")\n\n        texts = [\n            \"The defendant was charged with violation of contract terms.\",\n            \"Consumers have 30 days to return a defective product.\"\n        ]\n\n        embeddings = client.embed(texts=texts, model=\"vstackai-law-1\", is_query=False)\n\n        print(embeddings.embeddings.shape)  # (2, 1536)\n        ```\n    \"\"\" \n    # Validate input arguments\n    if not isinstance(texts, list) or not all(isinstance(text, str) for text in texts):\n        raise ValueError(\"'texts' must be a list of strings\")\n    if not isinstance(model, str):\n        raise ValueError(\"'model' must be a string\")\n    if not isinstance(is_query, bool):\n        raise ValueError(\"'is_query' must be a boolean\")\n    if not isinstance(instruction, str):\n        raise ValueError(\"'instruction' must be a string\")\n\n    for attempt in self.retry_controller:\n        with attempt:\n            response_json = api_resources.Embedding.encode(\n                texts=texts,\n                model=model,\n                is_query=is_query,\n                instruction=instruction,\n                connection_params=self.connection_params\n            )\n    return EmbeddingsObject(response_json, batch_size=len(texts))\n</code></pre>"},{"location":"precise_search/index.html","title":"PreciseSearch Documentation","text":""},{"location":"precise_search/index.html#introduction","title":"Introduction","text":"<p>PreciseSearch is a vertically integrated search engine developed by VectorStackAI. </p> <p>Unlike traditional retrieval systems, PreciseSearch provides a comprehensive search-as-a-service\u2014handling everything from embedding generation, vector storage, retrieval and re-ranking. This fully integrated approach ensures high-quality, efficient, and precise search results tailored to your needs.</p> <p>The documentation is divided into the four following sections:</p> <ol> <li>Quickstart: A quickstart guide to help you get started with PreciseSearch.</li> <li>How-To Guides: Practical guides for using PreciseSearch, including installation, index creation and management,  data upsertion and deletion, etc. These guides serve as a hands-on manual to help you integrate PreciseSearch into your workflow. </li> <li>Tutorials: Step-by-step guides demonstrating how to use PreciseSearch in real-world scenarios. This section provides practical tutorials that showcase PreciseSearch in action, helping you understand its functionality  while offering a template to kickstart your own applications. </li> <li>Reference: API reference for the PreciseSearch Python SDK.</li> </ol>"},{"location":"precise_search/how-to-guides.html","title":"How-to Guides for PreciseSearch","text":"<p>This document provides links to various how-to guides for tasks which can be performed using PreciseSearch.</p>"},{"location":"precise_search/how-to-guides.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation and Getting Started</li> <li>Creating Indexes<ul> <li>Dense Indexes</li> <li>Hybrid Indexes</li> </ul> </li> <li>Managing Data<ul> <li>Upserting Data</li> <li>Deleting Vectors</li> </ul> </li> <li>Searching<ul> <li>Required Arguments</li> <li>Detailed Search Scenarios</li> </ul> </li> <li>Index Management<ul> <li>Getting Index Info</li> <li>Listing Indexes</li> <li>Optimizing Indexes for Latency</li> <li>Deleting Indexes</li> </ul> </li> </ul>"},{"location":"precise_search/quickstart.html","title":"PreciseSearch Quickstart","text":"<p>This guide will show you how to get started with PreciseSearch. </p> <p>Code for this quickstart</p> <p>You can find and run the code for this quickstart in Colab Notebook</p>"},{"location":"precise_search/quickstart.html#1-install-the-sdk","title":"1. Install the SDK","text":"<p>PreciseSearch is a search-as-a-service product by VectorStackAI, designed to provide vector search capabilities.  Currently, we offer a Python SDK to interact with the PreciseSearch service. To get started, install the VectorStackAI Python SDK using pip:</p> <pre><code>pip install vectorstackai\n</code></pre>"},{"location":"precise_search/quickstart.html#2-get-an-api-key","title":"2. Get an API key","text":"<p>You will need an API key to use the SDK.  You can get it by signing up on the VectorStackAI website.</p>"},{"location":"precise_search/quickstart.html#3-create-an-index","title":"3. Create an index","text":"<p>PreciseSearch supports two types of indexes: Dense Indexes and Hybrid Indexes. Dense indexes search over dense vector representations of the data, where  as hybrid indexes search over a combination of dense  and sparse representations of the data. </p> <p>In this quickstart, you will create a dense index with an integrated embedding model (hosted by VectorStackAI). With dense indexes configured with an integrated embedding model, during upsert and search, you only need to provide the text data. PreciseSearch will generate the dense vector representations of the data automatically.</p> <pre><code>from vectorstackai import PreciseSearch\nimport time\n\n# Initialize the client with your API key\nclient = PreciseSearch(api_key=\"your_api_key\")\n\n# Create the index with an integrated embedding model (e.g., 'e5-small-v2')\nclient.create_index(index_name=\"my_dense_index\", embedding_model_name=\"e5-small-v2\")\n\n# Wait for the index to be ready\nwhile client.index_status(index_name=\"my_dense_index\") != \"ready\":\n    time.sleep(2)\n    print(\"Index is not ready yet. Waiting for 2 seconds...\")\n\nindex = client.connect_to_index(index_name=\"my_dense_index\")\n</code></pre> <p>Learn More</p> <p>To read about the different types of indexes you can create with PreciseSearch, see the Creating Indexes guide.</p>"},{"location":"precise_search/quickstart.html#4-upsert-data-into-the-index","title":"4. Upsert Data into the Index","text":"<p>To upsert data into the index, you need to provide the text data and assign a unique ID to each data point.</p> <p>For this example, let's create a dataset with 10 random facts about food and history.</p> <pre><code>dataset = [\n    {\"id\": \"1\", \"text\": \"The shortest war in history was between Britain and Zanzibar on August 27, 1896. Zanzibar surrendered after just 38 minutes.\"},\n    {\"id\": \"2\", \"text\": \"Ancient Romans used bread as plates. After the meal, these edible plates were either eaten or given to the poor.\"},\n    {\"id\": \"3\", \"text\": \"The first chocolate bar was made in England by Fry's in 1847, marking the beginning of the modern chocolate industry.\"},\n    {\"id\": \"4\", \"text\": \"The Battle of Hastings in 1066 changed English history forever when William the Conqueror defeated King Harold II.\"},\n    {\"id\": \"5\", \"text\": \"The Great Wall of China took over 2000 years to build, with construction starting in the 7th century BCE.\"},\n    {\"id\": \"6\", \"text\": \"Ketchup was sold as medicine in the 1830s to treat diarrhea, indigestion, and other stomach problems.\"},\n    {\"id\": \"7\", \"text\": \"Pizza was invented in Naples, Italy in the late 1700s. The classic Margherita pizza was created in 1889.\"},\n    {\"id\": \"8\", \"text\": \"The first Thanksgiving feast in 1621 lasted for three days and included deer, fish, and wild fowl.\"},\n    {\"id\": \"9\", \"text\": \"The signing of the Magna Carta in 1215 limited the power of English monarchs and influenced modern democracy.\"},\n    {\"id\": \"10\", \"text\": \"During World War II, carrots were promoted by the British as helping pilots see better at night to hide radar technology.\"}\n]\n\n# Parse the ids and texts for batch upsert\nbatch_ids = [item['id'] for item in dataset] \nbatch_metadata = [{'text': item['text']} for item in dataset] \n\n# Upsert the data into the index\nindex.upsert(batch_ids=batch_ids, \n             batch_metadata=batch_metadata)\n\n# Once the upsert is complete, you can check the number of vectors in the index via the index info\nindex_info = index.info()\nfor key, value in index_info.items():\n    print(f\"{key}: {value}\")\n</code></pre> <p>The index info will look like this: <pre><code>index_name: my_dense_index\nnum_records: 10\ndimension: 384\nmetric: dotproduct\nfeatures_type: dense\nstatus: ready\nembedding_model_name: e5-small-v2\noptimized_for_latency: False\n</code></pre></p> <p>Learn More</p> <p>For more details, see the How-To Guide on Upserting Data.</p>"},{"location":"precise_search/quickstart.html#5-search-the-index","title":"5. Search the index","text":"<p>Now that you have your index ready, you can search it. In this example, you will search the index for the query \"Where was pizza invented?\".</p> <p>Since the index is configured with an integrated embedding model, you only need to provide the query text. </p> <p><pre><code># Search the index\nsearch_results = index.search(query_text=\"Where was pizza invented?\", \n                              top_k=5)\n\n# Print the results\nfor result in search_results:\n    print(f\"ID: {result['id']}, Similarity: {result['similarity']:.2f}, Text: {result['metadata']['text']}\")\n</code></pre> Notice that the results are sorted in descending order of similarity, and contain results relevant to the query:</p> <pre><code>ID: 7, Similarity: 0.90, Text: Pizza was invented in Naples, Italy in the late 1700s. The classic Margherita pizza was created in 1889.\nID: 3, Similarity: 0.85, Text: The first chocolate bar was made in England by Fry's in 1847, marking the beginning of the modern chocolate industry.\nID: 5, Similarity: 0.80, Text: The Great Wall of China took over 2000 years to build, with construction starting in the 7th century BCE.\nID: 1, Similarity: 0.75, Text: The shortest war in history was between Britain and Zanzibar on August 27, 1896. Zanzibar surrendered after just 38 minutes.\n</code></pre> <p>Learn More</p> <p>For more details, see the How-To Guide on Searching the Index.</p>"},{"location":"precise_search/quickstart.html#6-clean-up","title":"6. Clean up","text":"<p>Once you are done with the quickstart, you can delete the index.</p> <pre><code>index.delete(ask_for_confirmation=False)\n</code></pre>"},{"location":"precise_search/reference.html","title":"Vector Index API Reference","text":""},{"location":"precise_search/reference.html#index-management-operations","title":"Index Management Operations","text":"<p>These operations are performed at the client level and don't require connecting to a specific index.</p>"},{"location":"precise_search/reference.html#vectorstackai.PreciseSearch.list_indexes","title":"vectorstackai.PreciseSearch.list_indexes","text":"<pre><code>list_indexes()\n</code></pre> <p>Lists information about all available indexes.</p> <p>Retrieves metadata for all indexes associated with the current API key.</p> <p>Returns:</p> Name Type Description <code>list_info</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, where each dictionary contains information about an index with the following keys:</p> <ul> <li>index_name (str): The name of the index.</li> <li>status (str): The current status of the index (\"initializing\" or \"ready\").</li> <li>num_records (int): The number of records stored in the index.</li> <li>dimension (int): The dimensionality of the vectors in the index.</li> <li>metric (str): The distance metric used for similarity search (\"cosine\" or \"dotproduct\").</li> <li>features_type (str): The type of features stored (\"dense\" or \"hybrid\").</li> <li>embedding_model_name (str): The name of the embedding model used (if applicable).</li> <li>optimized_for_latency (bool): Indicates whether the index is optimized for low-latency queries.</li> </ul> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def list_indexes(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Lists information about all available indexes.\n\n    Retrieves metadata for all indexes associated with the current API key.\n\n    Returns:\n        list_info: A list of dictionaries, where each dictionary contains information about an index with the following keys:\n\n            - index_name (str): The name of the index.\n            - status (str): The current status of the index (\"initializing\" or \"ready\").\n            - num_records (int): The number of records stored in the index.\n            - dimension (int): The dimensionality of the vectors in the index.\n            - metric (str): The distance metric used for similarity search (\"cosine\" or \"dotproduct\").\n            - features_type (str): The type of features stored (\"dense\" or \"hybrid\").\n            - embedding_model_name (str): The name of the embedding model used (if applicable).\n            - optimized_for_latency (bool): Indicates whether the index is optimized for low-latency queries.\n    \"\"\"\n    return api_resources.Index.list_indexes(connection_params=self.connection_params)\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.PreciseSearch.create_index","title":"vectorstackai.PreciseSearch.create_index","text":"<pre><code>create_index(index_name, embedding_model_name='none', dimension=None, metric='dotproduct', features_type='dense')\n</code></pre> <p>Creates a new vector index with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to create.</p> required <code>embedding_model_name</code> <code>str</code> <p>Name of the embedding model to use. There are two kinds of embedding models (integrated and non-integrated):</p> <ul> <li>Integrated models: These are pre-trained models hosted on the vector2search platform. Following models are supported: \"e5-small-v2\", \"e5-base-v2\", \"vstackai-law-1\", \"voyage-law-2\", \"openai-text-embedding-3-small\", \"openai-text-embedding-3-large\".</li> <li>Non-integrated models: These are custom models hosted on your platform/application. To use your own embedding model (i.e. non-integrated model), set \"embedding_model_name\" to \"none\".</li> </ul> <code>'none'</code> <code>dimension</code> <code>Optional[int]</code> <p>Vector dimension (required for non-integrated models).</p> <code>None</code> <code>metric</code> <code>Optional[str]</code> <p>Distance metric for comparing dense and sparse vectors. Must be one of \"cosine\" or \"dotproduct\".</p> <code>'dotproduct'</code> <code>features_type</code> <code>Optional[str]</code> <p>Type of features used in the index. Must be one of \"dense\" or \"hybrid\" (sparse + dense).</p> <code>'dense'</code> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def create_index(self, \n                 index_name: str, \n                 embedding_model_name: str = 'none', \n                 dimension: Optional[int] = None, \n                 metric: Optional[str] = 'dotproduct', \n                 features_type: Optional[str] = 'dense') -&gt; None:\n    \"\"\"Creates a new vector index with the specified parameters.\n\n    Args:\n        index_name: Name of the index to create.\n        embedding_model_name: Name of the embedding model to use. There are two kinds of embedding models (integrated and non-integrated):\n\n            - Integrated models: These are pre-trained models hosted on the vector2search platform. Following models are supported: \"e5-small-v2\", \"e5-base-v2\", \"vstackai-law-1\", \"voyage-law-2\", \"openai-text-embedding-3-small\", \"openai-text-embedding-3-large\".\n            - Non-integrated models: These are custom models hosted on your platform/application. To use your own embedding model (i.e. non-integrated model), set \"embedding_model_name\" to \"none\".\n        dimension: Vector dimension (required for non-integrated models).\n        metric: Distance metric for comparing dense and sparse vectors. Must be one of \"cosine\" or \"dotproduct\".\n        features_type: Type of features used in the index. Must be one of \"dense\" or \"hybrid\" (sparse + dense).\n    \"\"\"\n    # Convert \"None\" to \"none\"\n    if embedding_model_name == 'None':\n        embedding_model_name = 'none'\n\n    json_data = {\n        \"index_name\": index_name,\n        \"embedding_model_name\": embedding_model_name,\n        \"dimension\": dimension,\n        \"metric\": metric,\n        \"features_type\": features_type,\n    }\n    response_json = api_resources.Index.create_index(json_data=json_data, \n                                                     connection_params=self.connection_params)\n    print(f\"{response_json['detail']}\")\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.PreciseSearch.index_info","title":"vectorstackai.PreciseSearch.index_info","text":"<pre><code>index_info(index_name)\n</code></pre> <p>Retrieves information about a specific vector index.</p> <p>This method searches for the index specified by <code>index_name</code> within the list of available indexes.  If the index exists, it returns a dictionary containing information about the index. This method  is useful to get information about the index without having to connect to it.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to retrieve information for.</p> required <p>Returns:</p> Name Type Description <code>index_info</code> <code>Dict[str, Any]</code> <p>A dictionary containing information about the index with the following keys:</p> <ul> <li>index_name (str): The name of the index.</li> <li>status (str): The current status of the index (\"initializing\" or \"ready\").</li> <li>num_records (int): The number of records stored in the index.</li> <li>dimension (int): The dimensionality of the vectors in the index.</li> <li>metric (str): The distance metric used for similarity search.</li> <li>features_type (str): The type of features stored.</li> <li>embedding_model_name (str): The name of the embedding model used (if applicable).</li> <li>optimized_for_latency (bool): Whether the index is optimized for low-latency queries.</li> </ul> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def index_info(self, index_name: str) -&gt; Dict[str, Any]:\n    \"\"\"Retrieves information about a specific vector index.\n\n    This method searches for the index specified by `index_name` within the list of available indexes. \n    If the index exists, it returns a dictionary containing information about the index. This method \n    is useful to get information about the index without having to connect to it.\n\n    Args:\n        index_name: Name of the index to retrieve information for.\n\n    Returns:\n        index_info: A dictionary containing information about the index with the following keys:\n\n            - index_name (str): The name of the index.\n            - status (str): The current status of the index (\"initializing\" or \"ready\").\n            - num_records (int): The number of records stored in the index.\n            - dimension (int): The dimensionality of the vectors in the index.\n            - metric (str): The distance metric used for similarity search.\n            - features_type (str): The type of features stored.\n            - embedding_model_name (str): The name of the embedding model used (if applicable).\n            - optimized_for_latency (bool): Whether the index is optimized for low-latency queries.\n    \"\"\"\n    info_all_indexes = self.list_indexes()\n    for info_index in info_all_indexes:\n        if info_index['index_name'] == index_name:\n            return info_index\n\n    raise ValueError(f\"Index {index_name} not found in the list of existing indexes\")\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.PreciseSearch.index_status","title":"vectorstackai.PreciseSearch.index_status","text":"<pre><code>index_status(index_name)\n</code></pre> <p>Retrieves the status of a specific vector index.</p> <p>This method retrieves the status of the index specified by <code>index_name</code>. Here are the possible statuses:</p> <ul> <li>\"initializing\": The index is being initialized.</li> <li>\"ready\": The index is ready for use.</li> <li>\"failed\": The index failed to initialize.</li> <li>\"deleting\": The index is being deleted.</li> <li>\"undergoing_optimization_for_latency\": The index is undergoing optimization for better latency and throughput.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to retrieve status for.</p> required <p>Returns:</p> Name Type Description <code>index_status</code> <code>str</code> <p>The current status of the index.</p> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def index_status(self, index_name: str) -&gt; Dict[str, Any]:\n    \"\"\"Retrieves the status of a specific vector index.\n\n    This method retrieves the status of the index specified by `index_name`.\n    Here are the possible statuses:\n\n    - \"initializing\": The index is being initialized.\n    - \"ready\": The index is ready for use.\n    - \"failed\": The index failed to initialize.\n    - \"deleting\": The index is being deleted.\n    - \"undergoing_optimization_for_latency\": The index is undergoing optimization for better latency and throughput.\n\n    Args:\n        index_name: Name of the index to retrieve status for.\n\n    Returns:\n        index_status (str): The current status of the index.\n    \"\"\"\n    return self.index_info(index_name)['status']\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.PreciseSearch.connect_to_index","title":"vectorstackai.PreciseSearch.connect_to_index","text":"<pre><code>connect_to_index(index_name)\n</code></pre> <p>Connects to an existing vector index and returns an IndexObject for further operations.</p> <p>This method searches for the index specified by <code>index_name</code> within the list of available indexes.  If the index exists, it returns an <code>IndexObject</code> configured with the current connection parameters,  which can be used to perform operations such as upsert, search, and more on the index.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>The name of the index to connect to.</p> required <p>Returns:</p> Name Type Description <code>IndexObject</code> <code>IndexObject</code> <p>An object that provides methods to interact with the specified vector index.</p> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def connect_to_index(self, index_name: str) -&gt; IndexObject:\n    \"\"\"Connects to an existing vector index and returns an IndexObject for further operations.\n\n    This method searches for the index specified by `index_name` within the list of available indexes. \n    If the index exists, it returns an `IndexObject` configured with the current connection parameters, \n    which can be used to perform operations such as upsert, search, and more on the index.\n\n    Args:\n        index_name (str): The name of the index to connect to.\n\n    Returns:\n        IndexObject: An object that provides methods to interact with the specified vector index.\n    \"\"\" \n\n    info_all_indexes = self.list_indexes()\n    for index in info_all_indexes:\n        if index['index_name'] == index_name:\n            return IndexObject(index_name=index_name, connection_params=self.connection_params)\n\n    raise ValueError(f\"Index {index_name} not found in the list of existing indexes\")\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.PreciseSearch.delete_index","title":"vectorstackai.PreciseSearch.delete_index","text":"<pre><code>delete_index(index_name, ask_for_confirmation=True)\n</code></pre> <p>Deletes a vector index by its name.</p> <p>Permanently deletes the specified index and all its contents.  The deletion is asynchronous, and the deleted index cannot be recovered.  Note, this method is useful for deleting an index without having to connect to it.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>Name of the index to delete.</p> required <code>ask_for_confirmation</code> <code>bool</code> <p>Whether to ask for confirmation before deleting the index.</p> <code>True</code> Source code in <code>src/vectorstackai/client.py</code> <pre><code>def delete_index(self, index_name: str, ask_for_confirmation: bool = True) -&gt; None:\n    \"\"\"Deletes a vector index by its name.\n\n    Permanently deletes the specified index and all its contents. \n    The deletion is asynchronous, and the deleted index cannot be recovered. \n    Note, this method is useful for deleting an index without having to connect to it.\n\n    Args:\n        index_name (str): Name of the index to delete.\n        ask_for_confirmation (bool): Whether to ask for confirmation before deleting the index.\n    \"\"\"\n\n    # Ask the user to confirm the deletion\n    #########################################################\n    if ask_for_confirmation:\n        print(f\"Are you sure you want to delete index '{index_name}'? \"\n              f\"This action is irreversible.\")\n        confirm = input(\"Type 'yes' to confirm: \")\n        if confirm != 'yes':\n            print(\"Deletion cancelled.\")\n            return\n\n    response_json = api_resources.Index.delete_index(index_name=index_name, \n                                     connection_params=self.connection_params)\n    print(f\"{response_json['detail']}\")\n</code></pre>"},{"location":"precise_search/reference.html#index-operations","title":"Index Operations","text":"<p>These operations are performed on a specific index after connecting to it.</p>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.set_similarity_scale","title":"vectorstackai.objects.index.IndexObject.set_similarity_scale","text":"<pre><code>set_similarity_scale(dense_scale=1.0, sparse_scale=1.0)\n</code></pre> <p>Set the scale values for dense and sparse similarity scores in hybrid search.</p> <p>The similarity in a hybrid index is computed as a weighted sum of the dense and  sparse similarity scores:</p> <pre><code>similarity = dense_similarity * dense_scale + \n             sparse_similarity * sparse_scale\n</code></pre> <p>This method allows you to set the scale values for the dense and sparse similarity  scores. The scale values must be between 0 and 1.</p> Note <p>In a dense index, the scale values are ignored. The similarity is computed as:</p> <pre><code>similarity = dense_similarity.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dense_scale</code> <code>float</code> <p>The scale value for the dense similarity score. Defaults to 1.0.</p> <code>1.0</code> <code>sparse_scale</code> <code>float</code> <p>The scale value for the sparse similarity score. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def set_similarity_scale(self, \n                        dense_scale: float = 1.0, \n                        sparse_scale: float = 1.0) -&gt; None:\n    \"\"\"Set the scale values for dense and sparse similarity scores in hybrid search.\n\n    The similarity in a hybrid index is computed as a weighted sum of the dense and \n    sparse similarity scores:\n\n        similarity = dense_similarity * dense_scale + \n                     sparse_similarity * sparse_scale\n\n    This method allows you to set the scale values for the dense and sparse similarity \n    scores. The scale values must be between 0 and 1.\n\n    Note:\n        In a dense index, the scale values are ignored. The similarity is computed as:\n\n            similarity = dense_similarity.\n\n    Args:\n        dense_scale: The scale value for the dense similarity score.\n            Defaults to 1.0.\n        sparse_scale: The scale value for the sparse similarity score.\n            Defaults to 1.0.\n    \"\"\"\n    if self.features_type == 'dense':\n        warnings.warn(\"Setting scale values for dense and sparse features is redundant for dense indexes, since index is dense only; they will not be used for search..\")\n\n    # Validate scale values\n    if dense_scale == 0.0 and sparse_scale == 0.0:\n        raise ValueError(\"At least one of the scale values must be set to a non-zero value.\")\n    if dense_scale &lt; 0.0 or dense_scale &gt; 1.0:\n        raise ValueError(\"dense_scale must be between 0.0 and 1.0\")\n    if sparse_scale &lt; 0.0 or sparse_scale &gt; 1.0:\n        raise ValueError(\"sparse_scale must be between 0.0 and 1.0\")\n\n    self.dense_similarity_scale = dense_scale\n    self.sparse_similarity_scale = sparse_scale\n\n    if sparse_scale == 0.0:\n        warnings.warn(\"Sparse similarity scale is set to 0.0; sparse features will not be used for search..\")\n    if dense_scale == 0.0:\n        warnings.warn(\"Dense similarity scale is set to 0.0; dense features will not be used for search..\")\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.upsert","title":"vectorstackai.objects.index.IndexObject.upsert","text":"<pre><code>upsert(batch_ids, batch_metadata=None, batch_vectors=None, batch_sparse_values=None, batch_sparse_indices=None)\n</code></pre> <p>Upsert a batch of vectors and associated metadata to the index.</p> <p>This method upserts a batch of dense or sparse vectors, along with their associated metadata in the index. Note, if a datapoint with the same ID already exists, its metadata, vector, and sparse vector will be updated with the new values.</p> <p>Parameters:</p> Name Type Description Default <code>batch_ids</code> <code>List[str]</code> <p>List of unique identifiers for each vector.</p> required <code>batch_metadata</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of dictionaries containing metadata for each vector. For indexes configured  with an integrated embedding model, each dictionary should include a 'text' key whose  value is used to compute the embeddings.</p> <code>None</code> <code>batch_vectors</code> <code>Optional[List[List[float]]]</code> <p>List of dense vectors (each represented as a list of floats) corresponding to  each ID. This field is required for indexes configured with a non-integrated embedding  model and should be omitted for indexes configured with an integrated embedding model.</p> <code>None</code> <code>batch_sparse_values</code> <code>Optional[List[List[float]]]</code> <p>List of values in the sparse vector (each as a list of floats)  corresponding to each ID. Required for upserting in a hybrid index.</p> <code>None</code> <code>batch_sparse_indices</code> <code>Optional[List[List[int]]]</code> <p>List of indices in the sparse vector (each as a list of ints)  corresponding to each ID. Required for upserting in a hybrid index.</p> <code>None</code> Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def upsert(self, \n           batch_ids: List[str], \n           batch_metadata: Optional[List[Dict[str, Any]]] = None, \n           batch_vectors: Optional[List[List[float]]] = None, \n           batch_sparse_values: Optional[List[List[float]]] = None, \n           batch_sparse_indices: Optional[List[List[int]]] = None) -&gt; None:\n    \"\"\"Upsert a batch of vectors and associated metadata to the index.\n\n    This method upserts a batch of dense or sparse vectors, along with their associated metadata in the index. Note, if a datapoint with the same ID already exists, its metadata, vector, and sparse vector will be updated with the new values.\n\n    Args:\n        batch_ids: List of unique identifiers for each vector.\n        batch_metadata: List of dictionaries containing metadata for each vector. For indexes configured \n            with an integrated embedding model, each dictionary should include a 'text' key whose \n            value is used to compute the embeddings.\n        batch_vectors: List of dense vectors (each represented as a list of floats) corresponding to \n            each ID. This field is required for indexes configured with a non-integrated embedding \n            model and should be omitted for indexes configured with an integrated embedding model.\n        batch_sparse_values: List of values in the sparse vector (each as a list of floats) \n            corresponding to each ID. Required for upserting in a hybrid index.\n        batch_sparse_indices: List of indices in the sparse vector (each as a list of ints) \n            corresponding to each ID. Required for upserting in a hybrid index.\n    \"\"\"\n    # Validate input types\n    self._validate_upsert_input(batch_ids, batch_metadata, batch_vectors, batch_sparse_values, batch_sparse_indices) \n\n    json_data = {\n        \"ids\": batch_ids,\n        \"metadata\": batch_metadata,\n        \"vectors\": batch_vectors,\n        \"sparse_values\": batch_sparse_values,\n        \"sparse_indices\": batch_sparse_indices,\n    }\n    response = api_resources.Index.upsert(self.index_name, json_data, self.connection_params)\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.search","title":"vectorstackai.objects.index.IndexObject.search","text":"<pre><code>search(top_k=10, query_text=None, query_vector=None, query_sparse_values=None, query_sparse_indices=None, return_metadata=True)\n</code></pre> <p>Search the index for entries similar to the query.</p> <p>Finds entries in the index that are most similar to the provided query. Query can be a text (if using an integrated embedding model) or a dense vector (if using a non-integrated embedding model), along with a sparse vector (if using a hybrid index).</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top-k results to return (should be &gt;= 1).</p> <code>10</code> <code>query_text</code> <code>str</code> <p>Query text (required for integrated embedding models).</p> <code>None</code> <code>query_vector</code> <code>List[float]</code> <p>Query vector (required for non-integrated embedding models).</p> <code>None</code> <code>query_sparse_values</code> <code>List[float]</code> <p>Query sparse values (required for hybrid indexes).</p> <code>None</code> <code>query_sparse_indices</code> <code>List[int]</code> <p>Query sparse indices (required for hybrid indexes).</p> <code>None</code> <code>return_metadata</code> <code>bool</code> <p>Whether to return metadata for each result (optional, defaults to True).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>search_results</code> <code>Dict[str, Any]</code> <p>List of dictionaries containing search results, sorted in descending order of similarity scores. Each dictionary contains:</p> <ul> <li>id (str): ID of the retrieved vector.</li> <li>similarity (float): Similarity score between the query and the retrieved vector.</li> <li>metadata (dict): Metadata associated with the vector (present if return_metadata=True, otherwise defaults to an empty dict).</li> </ul> Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def search(self, \n           top_k: int = 10, \n           query_text: str = None, \n           query_vector: List[float] = None, \n           query_sparse_values: List[float] = None,\n           query_sparse_indices: List[int] = None,\n           return_metadata: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"Search the index for entries similar to the query.\n\n    Finds entries in the index that are most similar to the provided query. Query can be a text (if using an integrated embedding model) or a dense vector (if using a non-integrated embedding model), along with a sparse vector (if using a hybrid index).\n\n    Args:\n        top_k: Number of top-k results to return (should be &gt;= 1).\n        query_text: Query text (required for integrated embedding models).\n        query_vector: Query vector (required for non-integrated embedding models).\n        query_sparse_values: Query sparse values (required for hybrid indexes).\n        query_sparse_indices: Query sparse indices (required for hybrid indexes).\n        return_metadata: Whether to return metadata for each result (optional, defaults to True).\n\n    Returns:\n        search_results: List of dictionaries containing search results, sorted in descending order of similarity scores. Each dictionary contains:\n\n            - id (str): ID of the retrieved vector.\n            - similarity (float): Similarity score between the query and the retrieved vector.\n            - metadata (dict): Metadata associated with the vector (present if return_metadata=True, otherwise defaults to an empty dict).\n    \"\"\"\n    self._validate_search_input(top_k, query_text, query_vector, query_sparse_values, query_sparse_indices)\n    json_data = {\n        \"top_k\": top_k,\n        \"query_text\": query_text,\n        \"return_metadata\": return_metadata,\n        \"query_vector\": query_vector,\n        \"query_sparse_values\": query_sparse_values,\n        \"query_sparse_indices\": query_sparse_indices,\n        \"dense_similarity_scale\": self.dense_similarity_scale,\n        \"sparse_similarity_scale\": self.sparse_similarity_scale\n    }\n    response = api_resources.Index.search(self.index_name, json_data, self.connection_params)\n    return response['search_results']\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.info","title":"vectorstackai.objects.index.IndexObject.info","text":"<pre><code>info()\n</code></pre> <p>Get information about the index.</p> <p>If the index is still being created (i.e., not yet ready), the returned dictionary  includes only <code>\"index_name\"</code> and a <code>\"status\"</code> (which is <code>\"initializing\"</code>). Once the index is configured, the status is set to<code>\"ready\"</code>, the returned dictionary includes additional information.</p> <p>Returns:</p> Name Type Description <code>index_info</code> <code>Dict[str, Any]</code> <p>A dictionary containing information about the index with the following keys:</p> <ul> <li>index_name (str): The name of the index.</li> <li>status (str): The current status of the index (\"initializing\" or \"ready\").</li> <li>num_records (int): The number of records stored in the index.</li> <li>dimension (int): The dimensionality of the vectors in the index.</li> <li>metric (str): The distance metric used for similarity search (\"cosine\" or \"dotproduct\").</li> <li>features_type (str): The type of features stored (\"dense\" or \"hybrid\").</li> <li>embedding_model_name (str): The name of the embedding model used (if applicable).</li> <li>optimized_for_latency (bool): Indicates whether the index is optimized for low-latency queries.</li> </ul> Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get information about the index.\n\n    If the index is still being created (i.e., not yet ready), the returned dictionary \n    includes only `\"index_name\"` and a `\"status\"` (which is `\"initializing\"`). Once the index is configured, the status is set to`\"ready\"`, the returned dictionary includes additional information.\n\n    Returns:\n        index_info: A dictionary containing information about the index with the following keys:\n\n            - index_name (str): The name of the index.\n            - status (str): The current status of the index (\"initializing\" or \"ready\").\n            - num_records (int): The number of records stored in the index.\n            - dimension (int): The dimensionality of the vectors in the index.\n            - metric (str): The distance metric used for similarity search (\"cosine\" or \"dotproduct\").\n            - features_type (str): The type of features stored (\"dense\" or \"hybrid\").\n            - embedding_model_name (str): The name of the embedding model used (if applicable).\n            - optimized_for_latency (bool): Indicates whether the index is optimized for low-latency queries.\n    \"\"\"\n    return api_resources.Index.info(self.index_name, self.connection_params)\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.delete","title":"vectorstackai.objects.index.IndexObject.delete","text":"<pre><code>delete(ask_for_confirmation=True)\n</code></pre> <p>Deletes the vector index.</p> <p>Permanently deletes the index and all its contents. The deletion is asynchronous,  and the deleted index cannot be recovered. </p> <p>Parameters:</p> Name Type Description Default <code>ask_for_confirmation</code> <code>bool</code> <p>Whether to ask for confirmation before deleting the index. Defaults to True. When True, the user will be prompted to type 'yes' to confirm deletion.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the index doesn't exist or cannot be deleted.</p> Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def delete(self, ask_for_confirmation: bool = True) -&gt; None:\n    \"\"\"Deletes the vector index.\n\n    Permanently deletes the index and all its contents. The deletion is asynchronous, \n    and the deleted index cannot be recovered. \n\n    Args:\n        ask_for_confirmation (bool): Whether to ask for confirmation before deleting the index. Defaults to True. When True, the user will be prompted to type 'yes' to confirm deletion.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the index doesn't exist or cannot be deleted.\n    \"\"\"\n    # Ask the user to confirm the deletion\n    #########################################################\n    if ask_for_confirmation:\n        print(f\"Are you sure you want to delete index '{self.index_name}'? \"\n              f\"This action is irreversible.\")\n        confirm = input(\"Type 'yes' to confirm: \")\n        if confirm != 'yes':\n            print(\"Deletion cancelled.\")\n            return \n\n    api_resources.Index.delete_index(self.index_name, \n                                     self.connection_params)\n    print(f\"Request accepted: Index '{self.index_name}' deletion scheduled.\")\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.delete_vectors","title":"vectorstackai.objects.index.IndexObject.delete_vectors","text":"<pre><code>delete_vectors(ids)\n</code></pre> <p>Deletes vectors from the index by their IDs.</p> <p>Permanently removes the specified vectors from the index based on their unique identifiers. The deletion operation cannot be undone. This operation is performed synchronously. Even if one of the IDs does not exist, the operation will raise an error, and the index state will remain unchanged (i.e., no vectors will be deleted).</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>A list of string IDs identifying the vectors to delete from the index. Each ID must correspond to a vector previously added to the index.</p> required Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def delete_vectors(self, ids: List[str]) -&gt; None:\n    \"\"\"Deletes vectors from the index by their IDs.\n\n    Permanently removes the specified vectors from the index based on their unique identifiers. The deletion operation cannot be undone. This operation is performed synchronously. Even if one of the IDs does not exist, the operation will raise an error, and the index state will remain unchanged (i.e., no vectors will be deleted).\n\n    Args:\n        ids: A list of string IDs identifying the vectors to delete from the index.\n            Each ID must correspond to a vector previously added to the index.\n    \"\"\"\n    assert isinstance(ids, list), \"ids must be a list\"\n    assert len(ids) &gt; 0, \"ids must be a non-empty list\"\n    assert [isinstance(id, str) for id in ids], \"each element of ids must be a string\"\n\n    json_data = {\n        \"delete_vector_ids\": ids,\n    }\n    api_resources.Index.delete_vectors(self.index_name, json_data, self.connection_params)\n    print(f\"Successfully deleted {len(ids)} vectors from index {self.index_name}\")\n</code></pre>"},{"location":"precise_search/reference.html#vectorstackai.objects.index.IndexObject.optimize_for_latency","title":"vectorstackai.objects.index.IndexObject.optimize_for_latency","text":"<pre><code>optimize_for_latency()\n</code></pre> <p>Optimizes the index for better latency and throughput.</p> <p>This method triggers an optimization process in the background to improve the latency and throughput of the index for search operations.</p> Source code in <code>src/vectorstackai/objects/index.py</code> <pre><code>def optimize_for_latency(self) -&gt; None:\n    \"\"\"\n    Optimizes the index for better latency and throughput.\n\n    This method triggers an optimization process in the background to improve the latency and throughput of the index for search operations.\n    \"\"\"\n    api_resources.Index.optimize_for_latency(self.index_name, self.connection_params)\n    print(f\"Request accepted: Index '{self.index_name}' optimization scheduled.\")\n</code></pre>"},{"location":"precise_search/tutorials.html","title":"Tutorials","text":"<p>In this section, you will find a set of tutorials that cover the basics of PreciseSearch.  These tutorials will guide you through a set of complete examples/workflows.  These practical examples will show you how to use the library, and also provide a template to start building your own applications.</p> <p>Prerequisites</p> <p>The tutorials assume you have already have a valid API key.  If you don't have one yet, you can get it by signing up on the VectorStackAI.</p>"},{"location":"precise_search/tutorials.html#examples","title":"Examples","text":"<p>Below is a list of the available tutorials:</p> <ul> <li>Semantic search with VectorStackAI's integrated embedding model</li> <li>Semantic search with OpenAI's embedding model</li> <li>Semantic search with embedding model from Sentence Transformers</li> <li>Hybrid search</li> </ul>"},{"location":"precise_search/how_to_guides/creating-indexes.html","title":"Creating Indexes","text":"<p>PreciseSearch supports two types of indexes: Dense and Hybrid.</p> <ul> <li>Dense Indexes: Store and search data using dense embeddings.</li> <li>Hybrid Indexes: Combine dense embeddings with sparse embeddings to improve search relevance, especially for keyword-based queries.</li> </ul>"},{"location":"precise_search/how_to_guides/creating-indexes.html#dense-indexes","title":"Dense Indexes","text":"<p>Dense indexes are the most common type of vector index. They store embeddings as floating-point arrays.  When creating a dense index, you have two choices for generating and handling dense embeddings:</p> <ol> <li> <p>Integrated Embeddings</p> <p>The index automatically generates embeddings during upsert and search (using a built-in model from VectorStack AI).</p> </li> <li> <p>Non-Integrated Embeddings</p> <p>You manage the embeddings yourself\u2014meaning you supply embeddings during both upsert and search.</p> </li> </ol>"},{"location":"precise_search/how_to_guides/creating-indexes.html#using-an-integrated-embedding-model","title":"Using an Integrated Embedding Model","text":"<p>When you create a dense index with an integrated embedding model, you do not need to specify the vector dimension. The index will automatically:</p> <ul> <li>Generate dense embeddings from text during upserts.</li> <li>Generate embeddings for query text during searches.</li> </ul> <p>The example below creates a dense index with an integrated embedding model (e.g., <code>e5-small-v2</code>). The list of supported embedding models can be found here.</p> Creating a dense index with an integrated embedding model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Create the index\nclient.create_index(\n    index_name=\"my_dense_index\",\n    embedding_model_name=\"e5-small-v2\",  # Built-in embedding model\n    metric=\"cosine\",  # Similarity metric: \"cosine\" or \"dotproduct\"\n    features_type=\"dense\"\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/creating-indexes.html#using-a-non-integrated-embedding-model","title":"Using a Non-Integrated Embedding Model","text":"<p>If you prefer to manage your embeddings, set <code>embedding_model_name=\"none\"</code> and explicitly provide:</p> <ul> <li>The dimensionality <code>(dimension)</code> of your vectors.</li> <li>Dense embeddings during upsert and search.</li> </ul> Creating a dense index with a non-integrated embedding model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Create the index\nclient.create_index(\n    index_name=\"my_dense_index\",\n    embedding_model_name=\"none\",  # Indicates you'll provide your own vectors\n    dimension=384,  # Specify the dimension of your vectors\n    metric=\"cosine\",\n    features_type=\"dense\"\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/creating-indexes.html#hybrid-indexes","title":"Hybrid Indexes","text":"<p>Hybrid indexes represent each data point with both a dense embedding and a sparse embedding:</p> <ul> <li>Dense embeddings capture semantic similarity.</li> <li>Sparse embeddings (e.g., BM25, TF-IDF, eg. Splade) capture exact or near-exact term matches.</li> </ul> <p>By combining these two representations, hybrid indexes often yield better search relevance, since they account for both semantic meaning and keyword importance.</p> <p>Performance</p> <p>For performance reasons, hybrid indexes only support the <code>dotproduct</code> metric.</p>"},{"location":"precise_search/how_to_guides/creating-indexes.html#choosing-an-embedding-strategy","title":"Choosing an Embedding Strategy","text":"<p>Similar to dense indexes, hybrid indexes can use either integrated or non-integrated embedding models for the dense embeddings:</p> <ol> <li>Integrated:      The index automatically handles dense embeddings (you only provide sparse embeddings explicitly).</li> <li>Non-integrated:      You provide your own dense embeddings (in addition to the sparse embeddings).</li> </ol> <p>Sparse Embeddings</p> <p>You must always provide the sparse embeddings yourself for hybrid indexes.</p>"},{"location":"precise_search/how_to_guides/creating-indexes.html#using-an-integrated-dense-embedding-model","title":"Using an Integrated Dense Embedding Model","text":"<p>If you rely on a built-in dense embedding model (e.g., <code>e5-small-v2</code>), you do not need to specify the dimension. Sparse embeddings must still be provided by the user.</p> Creating a hybrid index with an integrated dense embedding model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\nclient.create_index(\n    index_name=\"my_hybrid_index\",\n    embedding_model_name=\"e5-small-v2\",  # Built-in dense embedding model\n    metric=\"dotproduct\",                 # Hybrid indexes support only \"dotproduct\" metric\n    features_type=\"hybrid\"\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/creating-indexes.html#using-a-non-integrated-dense-embedding-model","title":"Using a Non-Integrated Dense Embedding Model","text":"<p>For cases where you supply your own dense embeddings, set <code>embedding_model_name=\"none\"</code> and specify the <code>dimension</code>.  You will provide both the dense and sparse vectors during upsert and search.</p> Creating a hybrid index with a non-integrated dense embedding model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\nclient.create_index(\n    index_name=\"my_custom_hybrid_index\",\n    embedding_model_name=\"none\",  # Indicates user-provided dense embeddings\n    dimension=384,                # Specify vector dimensionality\n    metric=\"dotproduct\",          # Hybrid indexes support only \"dotproduct\" metric\n    features_type=\"hybrid\"\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/creating-indexes.html#summary","title":"Summary","text":"<ol> <li> <p>Dense Indexes:</p> <ul> <li>Have two options for dense embeddings:<ul> <li>Integrated model (<code>embedding_model_name=\"e5-small-v2\", etc.</code>)<ul> <li>No need to specify dimension during upsert and search.</li> </ul> </li> <li>Non-integrated model (<code>embedding_model_name=\"none\"</code>)<ul> <li>You provide the dimension and the vectors during upsert and search.</li> </ul> </li> </ul> </li> <li>Both <code>cosine</code> or <code>dotproduct</code> metrics are supported.</li> </ul> </li> <li> <p>Hybrid Indexes:</p> <ul> <li>Combines dense + sparse vectors for better search relevance.</li> <li>Similarity is computed as a weighted sum of dense and sparse similarity scores.</li> <li>Dense embeddings can be integrated or user-provided.</li> <li>Sparse embeddings are always user-provided.</li> <li>Only the <code>dotproduct</code> metric is supported.</li> </ul> </li> </ol> <p>Use these guidelines when creating indexes in PreciseSearch to leverage the  appropriate embedding approach for your use case.</p>"},{"location":"precise_search/how_to_guides/index-management.html","title":"Index Management","text":"<p>This section covers various operations for managing your indexes, including getting information about specific indexes, listing all indexes, optimizing indexes for performance, and deleting indexes.</p>"},{"location":"precise_search/how_to_guides/index-management.html#getting-index-info","title":"Getting Index Info","text":"<p>There are two ways to get detailed information about a specific index:</p> <ol> <li> <p>Using the client's <code>index_info()</code> method: Getting information using the client<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Get info about a specific index\nindex_info = client.index_info(\"my_index_name\")\n\n# Access specific properties\nprint(f\"Status: {index_info['status']}\")\nprint(f\"Number of records: {index_info['num_records']}\")\nprint(f\"Dimension: {index_info['dimension']}\")\nprint(f\"Metric: {index_info['metric']}\")\n</code></pre></p> </li> <li> <p>Using the index object's <code>info()</code> method: Getting information using the index object<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to an existing index\nindex = client.connect_to_index(\"my_index_name\")\n\n# Get info about the index\nindex_info = index.info()\n\n# Access specific properties\nprint(f\"Status: {index_info['status']}\")\nprint(f\"Number of records: {index_info['num_records']}\")\n</code></pre></p> </li> </ol> <p>Both methods return a dictionary containing metadata including:</p> <ul> <li><code>index_name</code>: Name of the index</li> <li><code>status</code>: Current status (\"initializing\" or \"ready\")</li> <li><code>num_records</code>: Number of vectors stored in the index</li> <li><code>dimension</code>: Vector dimension</li> <li><code>metric</code>: Distance metric used (\"cosine\" or \"dotproduct\")</li> <li><code>features_type</code>: Type of features stored (\"dense\" or \"hybrid\")</li> <li><code>embedding_model_name</code>: Name of the integrated embedding model (if any)</li> <li><code>optimized_for_latency</code>: Whether the index is optimized for low-latency queries</li> </ul> <p>Index information during creation</p> <p>If the index is still being created (status is <code>\"initializing\"</code>), only basic information will be available. </p> <p>Once the index is ready, you will have access to all metadata fields.</p>"},{"location":"precise_search/how_to_guides/index-management.html#checking-index-status","title":"Checking Index Status","text":"<p>The <code>index_status()</code> method returns the current status of an index:</p> Checking index status<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Get the status of an index\nstatus = client.index_status(\"my_index_name\")\n\n# Print the status\nprint(f\"Index status: {status}\")\n</code></pre> <p>Possible statuses</p> <p>The possible statuses are:</p> <ul> <li>\"initializing\": The index is being initialized.</li> <li>\"ready\": The index is ready for use.</li> <li>\"failed\": The index failed to initialize.</li> <li>\"deleting\": The index is being deleted.</li> </ul> <ul> <li>\"undergoing_optimization_for_latency\": The index is undergoing optimization for better latency and throughput.</li> </ul>"},{"location":"precise_search/how_to_guides/index-management.html#listing-indexes","title":"Listing Indexes","text":"<p>The <code>list_indexes()</code> method returns information about all indexes associated with your API key:</p> Listing all indexes<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Get a list of all indexes\nindexes = client.list_indexes()\n\n# Print information about each index\nfor index in indexes:\n    print(f\"Index name: {index['index_name']}\")\n    print(f\"Status: {index['status']}\")\n    print(f\"Number of records: {index['num_records']}\")\n    print(\"---\")\n</code></pre> <p>Each index in the returned list contains the same metadata fields as described in the Getting Index Info section.</p>"},{"location":"precise_search/how_to_guides/index-management.html#optimizing-for-latency","title":"Optimizing For Latency","text":""},{"location":"precise_search/how_to_guides/index-management.html#introduction","title":"Introduction","text":"<p>When your dataset grows large, searching for the nearest neighbors in high-dimensional vector spaces becomes increasingly resource-intensive. To address this challenge, most modern vector databases use approximate nearest neighbor (ANN) search algorithms. These algorithms are typically built on specialized data structures\u2014such as Inverted File Index (IVF), Hierarchical Navigable Small World (HNSW), or Vamana graphs\u2014that enable fast lookups while preserving a high level of accuracy.</p>"},{"location":"precise_search/how_to_guides/index-management.html#latency-optimization-with-no-manual-tuning","title":"Latency Optimization with No Manual Tuning","text":"<p>Most vector databases require manual configuration of various ANN parameters, such as nprobe for IVF or the number of edges per node in HNSW. By contrast, PreciseSearch is designed to automatically select the best data structure and optimize its parameters, eliminating the need for manual tuning and simplifying setup.</p> <p>To optimize for latency in PreciseSearch, you can use the <code>optimize_for_latency()</code> method. This method runs an optimization process on the backend to reduce search latency.</p> <ul> <li>When to Call:  You usually need to call this method only once, ideally after you have loaded most of your data.  If the size of your index is less than 500,000 vectors, you do not need to call this method.</li> <li>Effect on Indexing:  You can still insert additional data afterward, but you will not need to run the optimization again.</li> <li>Asynchronous Execution:  Because this operation runs in the background, the method returns immediately while optimization continues behind the scenes.</li> </ul> Optimizing an index for latency<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to an existing index\nindex = client.connect_to_index(\"my_index_name\")\n\n# Optimize the index for better search performance\nindex.optimize_for_latency()\n</code></pre>"},{"location":"precise_search/how_to_guides/index-management.html#deleting-indexes","title":"Deleting Indexes","text":"<p>Important considerations when deleting indexes</p> <ul> <li>Deletion is permanent and cannot be undone</li> <li>By default, both methods will ask for confirmation before deletion</li> <li>You can bypass the confirmation by setting <code>ask_for_confirmation=False</code>. </li> <li>Be extremely careful when disabling confirmation, as deleted indexes cannot be recovered.</li> </ul> <p>There are two ways to delete an index:</p> <ol> <li> <p>Using the client's <code>delete_index()</code> method: Deleting using the client<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Delete an index (will ask for confirmation)\nclient.delete_index(\"my_index_name\")\n\n# Or delete without confirmation\nclient.delete_index(\"my_index_name\", ask_for_confirmation=False)\n</code></pre></p> </li> <li> <p>Using the index object's <code>delete()</code> method: Deleting using the index object<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to an existing index\nindex = client.connect_to_index(\"my_index_name\")\n\n# Delete the index (will ask for confirmation)\nindex.delete()\n\n# Or delete without confirmation\nindex.delete(ask_for_confirmation=False)\n</code></pre></p> </li> </ol>"},{"location":"precise_search/how_to_guides/installation.html","title":"Installation and Getting Started","text":""},{"location":"precise_search/how_to_guides/installation.html#installation","title":"Installation","text":"<p>PreciseSearch is a search-as-a-service product by VectorStackAI, designed to provide seamless vector search capabilities.  Currently, we offer a Python SDK to interact with the PreciseSearch service. To get started, install the VectorStackAI Python SDK using pip:</p> Installing the VectorStackAI Python SDK<pre><code>pip install vectorstackai\n</code></pre>"},{"location":"precise_search/how_to_guides/installation.html#getting-started","title":"Getting Started","text":"<p>Once you have installed the VectorStackAI Python SDK, to use the VectorStack AI SDK, you will need an API key.  You can obtain it by signing up on our website. Once, you have the API key, you can either set it as an environment variable or pass it directly when initializing the client:</p>"},{"location":"precise_search/how_to_guides/installation.html#option-1-pass-api-key-directly","title":"Option 1: Pass API Key Directly","text":"Initializing the client with an API key<pre><code>from vectorstackai import PreciseSearch\n\n# Initialize the client with your API key\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n</code></pre>"},{"location":"precise_search/how_to_guides/installation.html#option-2-set-api-key-as-environment-variable","title":"Option 2: Set API Key as Environment Variable","text":"<p>If you've set the <code>VECTORSTACK_API_KEY</code> environment variable, you can initialize the client without explicitly passing the API key: Initializing the client using an environment variable<pre><code>from vectorstackai import PreciseSearch\n\n# Initialize the client using the API key from environment variables\nclient = PreciseSearch()\n</code></pre> This allows for better security and flexibility when managing API credentials. </p>"},{"location":"precise_search/how_to_guides/managing-data.html","title":"Managing Data","text":"<p>This section describes how to manage data in PreciseSearch, specifically how to upsert and delete data.</p>"},{"location":"precise_search/how_to_guides/managing-data.html#upserting-data","title":"Upserting Data","text":"<p>Upserting is an operation that combines insert and update, allowing you to add new vectors or update existing ones based on their Unique ID. When upserting a vector with an ID that already exists in the index, the existing vector and its metadata will be updated with the new values. For new IDs, a new vector will be inserted into the index.</p> <p>Batching</p> <p>To improve performance, our SDK supports batching, enabling you to upsert multiple data points in a single call.</p> <p>On a high level, you need following information for each item for an upsert operation:</p> <ul> <li> <p>Unique ID (string)</p> <ul> <li>Required for all items. Must be unique within the index.</li> <li>Used to identify items for retrieval, updates, and deletion.</li> <li>Examples: <code>\"doc_123\"</code>, <code>\"product_456\"</code>, <code>\"user_789\"</code></li> </ul> </li> <li> <p>Dense Vector (required for non-integrated models)</p> <ul> <li>Dense numerical representation of the item, represented as a list of floats.</li> <li>Must match the dimension specified when creating the index.</li> <li>Example: <code>[0.1, 0.2, ..., 0.9]</code> (vector of specified dimension)</li> </ul> </li> <li> <p>Sparse Vector (required for hybrid indexes)</p> <ul> <li>Sparse representation of the item using two parallel lists:<ul> <li><code>sparse_values</code>: Non-zero values in the sparse vector, represented as a list of floats.</li> <li><code>sparse_indices</code>: Positions of those non-zero values, represented as a list of integers.</li> </ul> </li> <li>Example:     Specification of a sparse vector with 3 non-zero values<pre><code>sparse_values = [0.5, 0.8, 0.3]   # Non-zero values\nsparse_indices = [0, 3, 5]        # Their positions in the sparse vector\n</code></pre>     This represents a sparse vector where position 0 has value 0.5, position 3 has value 0.8, etc.</li> </ul> </li> <li> <p>Metadata (dictionary) </p> <ul> <li>Optional key-value pairs with additional item information.</li> <li>For indexes with integrated embedding models, must include a \"text\" field, whose value will be used for dense vector generation.</li> <li>Example: An example of metadata dictionary<pre><code>{\n    \"text\": \"Product description here\",\n    \"category\": \"Electronics\", \n    \"price\": 299.99,\n    \"in_stock\": True\n}\n</code></pre></li> </ul> </li> </ul> <p>The exact specifics/requirements of an upsert operation depend on your index type (dense or hybrid) and whether you are using an integrated or non-integrated embedding model. Below, we will go through the different scenarios in detail.</p>"},{"location":"precise_search/how_to_guides/managing-data.html#upserting-to-dense-indexes","title":"Upserting to Dense Indexes","text":"<p>TODO: Describe the high-leval requirement for upserting to dense indexes. Mirror that in the hybrid section.</p>"},{"location":"precise_search/how_to_guides/managing-data.html#dense-index-with-an-integrated-embedding-model","title":"Dense Index with an Integrated Embedding Model","text":"<p>To upsert data in a dense index configured with an integrated embedding model, you only need to provide following information for each item in the batch:</p> <ul> <li>IDs (unique for each item)</li> <li>Metadata (containing a \"text\" field)</li> </ul> <p>The index will automatically generate the dense vector from the \"text\" field.  You may include additional metadata (e.g., \"price\") for filtering or reference, but it will not affect the vector creation.</p> Upserting to a dense index with an integrated model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to an existing dense index with integrated model\nindex = client.connect_to_index(\"my_dense_index\")\n\n# Upsert data - dense vectors will be automatically generated from the 'text' field in metadata\nindex.upsert(\n    batch_ids=[\"1\", \"2\"],                 \n    batch_metadata=[                      \n        {\"text\": \"This is the first document to embed\", \"price\": 100},\n        {\"text\": \"This is the second document to embed\", \"price\": 200}\n    ]\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/managing-data.html#dense-index-with-a-non-integrated-embedding-model","title":"Dense Index with a Non-Integrated Embedding Model","text":"<p>To upsert data in a dense index configured with a non-integrated embedding model, you must explicitly provide  following information for each item in the batch:</p> <ul> <li>IDs (unique for each item)</li> <li>Dense vectors (via <code>vectors</code>)</li> <li>Metadata (optional)</li> </ul> <p>Important: Ensure the dimensionality of each <code>vector</code> matches the dimension specified when you created the index.</p> Upserting to a dense index with a non-integrated model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to a dense index with non-integrated model\nindex = client.connect_to_index(\"my_dense_index\")\n\n# Upsert data with dense vectors specified explicitly\nindex.upsert(\n    batch_ids=[\"1\", \"2\"],                 \n    batch_vectors=[                      \n        [0.1, 0.2, 0.3, 0.4],            \n        [0.5, 0.6, 0.7, 0.8]\n    ],\n    batch_metadata=[                      \n        {\"price\": 100},\n        {\"price\": 200}\n    ]\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/managing-data.html#upserting-to-hybrid-indexes","title":"Upserting to Hybrid Indexes","text":"<p>When upserting to a hybrid index, you must provide both dense and sparse representations, along with a unique ID and any optional metadata.  The key difference from a dense index is the required sparse vector, which you must explicitly supply using <code>sparse_values</code> and <code>sparse_indices</code>. </p> <p>For the dense vector component in hybrid indexes, there are two options, just like with dense indexes:</p> <ul> <li>Automatically generated (if your index uses an integrated embedding model, in which case you only need to include text in the metadata), or</li> <li>Explicitly provided (if your index uses a non-integrated embedding model).</li> </ul>"},{"location":"precise_search/how_to_guides/managing-data.html#hybrid-index-with-an-integrated-embedding-model","title":"Hybrid Index with an Integrated Embedding Model","text":"<p>For hybrid indexes configured with an integrated embedding model (for the dense vectors),  you need to supply following information for each item in the batch:</p> <ul> <li>IDs</li> <li>Sparse vectors (via <code>sparse_values</code> and <code>sparse_indices</code>)</li> <li>Metadata (including a <code>\"text\"</code> field, from which dense vectors are automatically generated)</li> </ul> Upserting to a hybrid index with an integrated model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to a hybrid index with integrated model\nindex = client.connect_to_index(\"my_hybrid_index\")\n\n# Upsert data - dense vectors will be automatically generated from the 'text' field in metadata\nindex.upsert(\n    batch_ids=[\"1\", \"2\"],\n    batch_metadata=[\n        {\"text\": \"This is the first document to embed\", \"price\": 100},\n        {\"text\": \"This is the second document to embed\", \"price\": 200}\n    ],\n    batch_sparse_values=[[0.5, 0.8, 0.3], [0.2, 0.4, 0.6]],\n    batch_sparse_indices=[[0, 3, 5], [1, 2, 4]]\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/managing-data.html#with-non-integrated-embedding-model","title":"With Non-Integrated Embedding Model","text":"<p>For hybrid indexes configured with a non-integrated embedding model, you must explicitly provide:</p> <ul> <li>IDs</li> <li>Dense vectors (via <code>vectors</code>)</li> <li>Sparse vectors (via <code>sparse_values</code> and <code>sparse_indices</code>)</li> <li>Metadata (optional fields)</li> </ul> Upserting to a hybrid index with a non-integrated model<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to a hybrid index with non-integrated model\nindex = client.connect_to_index(\"my_hybrid_index\")\n\n# Upsert data with explicit vectors\nindex.upsert(\n    batch_ids=[\"1\", \"2\"],\n    batch_vectors=[[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]],\n    batch_sparse_values=[[0.5, 0.8, 0.3], [0.2, 0.4, 0.6]],\n    batch_sparse_indices=[[0, 3, 5], [1, 2, 4]],\n    batch_metadata=[\n        {\"price\": 100},\n        {\"price\": 200}\n    ]\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/managing-data.html#deleting-vectors","title":"Deleting Vectors","text":"<p>To remove vectors from the index, call <code>delete_vectors</code> with the <code>ids</code> of the vectors you want to delete. This operation:</p> <ul> <li>Is performed synchronously and cannot be undone.</li> <li>All provided <code>ids</code> must exist in the index, otherwise an error is raised and no vectors are deleted.</li> </ul> Deleting vectors by ID<pre><code>from vectorstackai import PreciseSearch\nclient = PreciseSearch(api_key=\"your_api_key_here\")\n\n# Connect to an existing index\nindex = client.connect_to_index(\"my_index\")\n\n# Delete vectors by ID\nindex.delete_vectors(ids=[\"doc1\", \"doc2\"])\n</code></pre>"},{"location":"precise_search/how_to_guides/searching.html","title":"Searching","text":"<p>The search method enables you to find the most relevant vectors in your index based on a query.  It returns a ranked list of the closest matches, sorted by similarity score.</p>"},{"location":"precise_search/how_to_guides/searching.html#response-format","title":"Response format","text":"<p>Each search result (in the returned list) contains:</p> <ul> <li><code>id</code> \u2013 The identifier of the matching document/vector.</li> <li><code>similarity</code> \u2013 The similarity score for the match (higher is typically more relevant).</li> <li><code>metadata</code> \u2013 Any additional metadata stored with the vector (only returned if <code>return_metadata=True</code>).</li> </ul> Example showing how to search an index and handle the list of results<pre><code>results = index.search(\n    query_text=\"What is a document?\",\n    top_k=5,\n    return_metadata=True\n)\n\nfor item in results:\n    print(\"ID:\", item[\"id\"])\n    print(\"Similarity:\", item[\"similarity\"])\n    if \"metadata\" in item:\n        print(\"Metadata:\", item[\"metadata\"])\n    print(\"---\")\n</code></pre>"},{"location":"precise_search/how_to_guides/searching.html#required-arguments","title":"Required Arguments","text":"<p>Depending on your exact setup, the required inputs for each search can vary.  Use this table to see which arguments you must provide for each combination of index type and embedding model.</p> Index Type Embedding Model Required Arguments Dense Integrated <code>query_text</code> Non-integrated <code>query_vector</code> Hybrid Integrated <code>query_text</code>, <code>query_sparse_values</code>, <code>query_sparse_indices</code> Non-integrated <code>query_vector</code>, <code>query_sparse_values</code>, <code>query_sparse_indices</code> <p>In the table above, the following terms apply:</p> <ul> <li> <p><code>query_text</code> (string)   A text-based query string. For indexes configured with an integrated embedding model, dense vector representation is automatically generated from this text. </p> </li> <li> <p><code>query_vector</code> (list of floats)   A list of floats representing the dense vector for the query. This is required for indexes configured with non-integrated embedding models, where you handle the dense vector generation yourself.</p> </li> <li> <p><code>query_sparse_values</code> (list of floats) and <code>query_sparse_indices</code> (list of ints)   The numerical values and their corresponding indices for a sparse representation of the query (e.g., TF-IDF, BM25, etc.). </p> </li> </ul>"},{"location":"precise_search/how_to_guides/searching.html#optional-arguments","title":"Optional Arguments","text":"<p>In addition to these required arguments, you may also specify following optional arguments:</p> <ul> <li> <p><code>top_k</code> (int, default=10)   The number of results to return. </p> </li> <li> <p><code>return_metadata</code> (bool, default=True)   When set to True, metadata for each result is returned.    If you only need IDs and similarity scores, set this to False to speed up the query.</p> </li> </ul>"},{"location":"precise_search/how_to_guides/searching.html#detailed-search-scenarios","title":"Detailed Search Scenarios","text":"<p>Below are code examples demonstrating how to search your index based on different configurations. Each example shows the required and optional arguments needed for successful searching.</p>"},{"location":"precise_search/how_to_guides/searching.html#dense-indexes","title":"Dense Indexes","text":""},{"location":"precise_search/how_to_guides/searching.html#with-integrated-embedding-model","title":"With Integrated Embedding Model","text":"<p>To search a dense index configured with an integrated embedding model, you only need to provide a <code>query_text</code>. The method will automatically generate a dense query vector from this text.</p> <ul> <li>Required: <code>query_text</code> </li> <li>Optional: <code>top_k</code>, <code>return_metadata</code></li> </ul> Searching a dense index with an integrated embedding model<pre><code># Search using query's text\nresults = index.search(\n    query_text=\"What is a document?\", \n    top_k=5\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/searching.html#with-non-integrated-embedding-model","title":"With Non-Integrated Embedding Model","text":"<p>To search a dense index configured with a non-integrated embedding model, you must explicitly provide the dense vector representation of the query (via <code>query_vector</code>).</p> <ul> <li>Required: <code>query_vector</code> </li> <li>Optional: <code>top_k</code>, <code>return_metadata</code></li> </ul> Searching a dense index with a non-integrated embedding model<pre><code># Search using query's dense vector representation\nresults = index.search(\n    query_vector=[0.1, 0.2, 0.3, 0.4],\n    top_k=5\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/searching.html#hybrid-indexes","title":"Hybrid Indexes","text":"<p>A hybrid index combines both dense and sparse vector representations to enhance search quality.  Dense vectors capture the semantic meaning of text (through continuous embeddings), while sparse vectors (e.g., TF-IDF or BM25) capture exact keyword matches.</p>"},{"location":"precise_search/how_to_guides/searching.html#similarity-score-in-a-hybrid-index","title":"Similarity Score in a Hybrid Index","text":"<p>When searching, the similarity score between a query and a document is computed as a weighted sum of the query\u2013document similarity in both dense and sparse vector spaces:</p> <ul> <li><code>dense_similarity</code>: reflects how similar is the query\u2019s dense representation to the document\u2019s dense representation</li> <li><code>sparse_similarity</code>: reflects how similar is the query\u2019s sparse representation to the document\u2019s sparse representation</li> </ul> Similarity score calculation in a hybrid index<pre><code>similarity_score = (\n    dense_similarity * dense_scale\n    + sparse_similarity * sparse_scale\n)\n</code></pre> <p>Here, <code>dense_scale</code> and <code>sparse_scale</code> are floating-point numbers between 0.0 and 1.0 that control the relative contribution of each representation.  By default both are set to 1.0, giving equal weight to the dense and sparse vectors. You can adjust these values as needed via the <code>index.set_similarity_scale()</code> method.  For example, to make the sparse representation twice as influential as the dense representation, set:</p> Setting the similarity scale for a hybrid index<pre><code>index.set_similarity_scale(dense_scale=0.5, sparse_scale=1.0)\n</code></pre> <p>Note: Setting weights to 0.0 for either representation will effectively disable that representation in the final similarity score.</p>"},{"location":"precise_search/how_to_guides/searching.html#with-integrated-embedding-model_1","title":"With Integrated Embedding Model","text":"<p>To search a hybrid index configured with an integrated embedding model, you need to provide the <code>query_text</code> and its sparse vector representation (via <code>query_sparse_values</code> and <code>query_sparse_indices</code>).  The dense vector will be automatically generated from the <code>query_text</code>.</p> <ul> <li>Required: <ul> <li><code>query_text</code> </li> <li><code>query_sparse_values</code> </li> <li><code>query_sparse_indices</code> </li> </ul> </li> <li>Optional: <code>top_k</code>, <code>return_metadata</code></li> </ul> Searching a hybrid index with an integrated embedding model<pre><code># Search using text query\nresults = index.search(\n    query_text=\"What is a document?\",\n    query_sparse_values=[0.5, 0.8, 0.3],\n    query_sparse_indices=[0, 3, 5],\n    top_k=5\n)\n</code></pre>"},{"location":"precise_search/how_to_guides/searching.html#with-non-integrated-embedding-model_1","title":"With Non-Integrated Embedding Model","text":"<p>To search a hybrid index configured with a non-integrated embedding model, you must explicitly provide the dense and sparse vector representations of the query. The dense vector is provided via <code>query_vector</code> and the sparse vector is provided via <code>query_sparse_values</code> and <code>query_sparse_indices</code> respectively.</p> <ul> <li>Required: <ul> <li><code>query_vector</code> </li> <li><code>query_sparse_values</code> </li> <li><code>query_sparse_indices</code> </li> </ul> </li> <li>Optional: <code>top_k</code>, <code>return_metadata</code></li> </ul> Searching a hybrid index with a non-integrated embedding model<pre><code># Search using text query\nresults = index.search(\n    query_vector=[0.1, 0.2, 0.3, 0.4],\n    query_sparse_values=[0.5, 0.8, 0.3],\n    query_sparse_indices=[0, 3, 5],\n    top_k=5\n)\n</code></pre>"}]}